{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shreyas/Desktop/Shreyas/toy_inverse_problem'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_state(file_state = 'state.txt'):\n",
    "    \n",
    "    with open(file_state, 'r') as reader:\n",
    "        content = reader.readlines()\n",
    "        M0 = float(content[0])\n",
    "        M1 = float(content[1])\n",
    "        \n",
    "    return M0, M1\n",
    "\n",
    "def read_gradient(file_grad = 'gradient.txt'):\n",
    "    \n",
    "    with open(file_grad, 'r') as reader:\n",
    "        content = reader.readlines()\n",
    "        M0b = float(content[0])\n",
    "        M1b = float(content[1])\n",
    "        \n",
    "    return M0b, M1b\n",
    "\n",
    "def write_state(M0, M1, file_state = 'state.txt'):\n",
    "    \n",
    "    with open(file_state, 'w') as writer:\n",
    "        writer.write(f\"{M0}\\n\")\n",
    "        writer.write(f\"{M1}\")\n",
    "        \n",
    "    return None\n",
    "\n",
    "def eval_gradient(M0, M1, file_gradient = 'gradient.txt', file_state = 'state.txt'):\n",
    "    write_state(M0, M1)\n",
    "    os.system(\"make -f Makefile clean; make -f Makefile; ./adjoint\")\n",
    "    \n",
    "    with open(file_gradient, 'r') as reader:\n",
    "        content = reader.readlines()\n",
    "        M0b = float(content[0])\n",
    "        M1b = float(content[1])\n",
    "    \n",
    "        \n",
    "    return M0b, M1b\n",
    "\n",
    "def eval_loss(M0, M1, file_loss = 'loss_inexact_line_search.txt', file_state = 'state.txt'):\n",
    "    write_state(M0, M1)\n",
    "    os.system(\"make -f Makefile_forward clean; make -f Makefile_forward; ./forward\")\n",
    "    \n",
    "    with open(file_loss, 'r') as reader:\n",
    "        content = reader.readlines()\n",
    "        J = float(content[0])\n",
    "        \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS_update_matrix(B, inv_B, y, s):\n",
    "    \n",
    "    B_times_s = np.matmul(B, s)\n",
    "    y_dot_s = np.inner(y, s)\n",
    "    \n",
    "    ##### UPDATE B #####\n",
    "    B_new = B + np.outer(y,y)/y_dot_s - np.outer(B_times_s, B_times_s)/np.inner(s, B_times_s)\n",
    "    \n",
    "    ##### UPDATE INV_B #####\n",
    "    I = np.eye(B.shape[0], dtype = float)\n",
    "    \n",
    "    left = I - np.outer(s, y)/ y_dot_s\n",
    "    right = I - np.outer(y, s)/ y_dot_s\n",
    "    inv_B_new = np.matmul(left, np.matmul(inv_B, right)) + np.outer(s, s) / y_dot_s\n",
    "    \n",
    "    eigs, _ = np.linalg.eig(B_new)\n",
    "    \n",
    "    for eig in eigs:\n",
    "        if(eig <= 0):\n",
    "            print(\"Hessian not Positive definite\")\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return B_new, inv_B_new\n",
    "\n",
    "def search_alpha(state, gradient, rho, alpha = 1, c1 = 1e-4, c2 = 0.9):\n",
    "    while True:\n",
    "        \n",
    "        f_old = eval_loss(state[0], state[1])\n",
    "        f_new = eval_loss(state[0] + alpha*rho[0], state[1] + alpha*rho[1])\n",
    "        \n",
    "        gradient_new = np.array(eval_gradient(state[0] + alpha*rho[0], state[1] + alpha*rho[1]))\n",
    "        #print((f_new - f_old) - c1*alpha*np.inner(rho, gradient), np.inner(rho, gradient_new) -  c2*np.inner(rho, gradient))\n",
    "        if (f_new - f_old) <= c1*alpha*np.inner(rho, gradient) and np.inner(rho, gradient_new) >=  c2*np.inner(rho, gradient): \n",
    "            break\n",
    "        else:\n",
    "            alpha = alpha/2\n",
    "    return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scicomp.stackexchange.com/questions/11323/effect-of-initial-guess-b-approximate-hessian-on-bfgs-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.25\n",
      "0.125\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0078125\n",
      "0.00390625\n",
      "0.001953125\n",
      "0.0009765625\n",
      "iter = 1; M0 = 0.02033114380278951; M1 = 0.0009822981888490488; Loss = 0.4963958949023225\n",
      "0.5\n",
      "0.25\n",
      "0.125\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0078125\n",
      "0.00390625\n",
      "0.001953125\n",
      "0.0009765625\n",
      "0.00048828125\n",
      "0.000244140625\n",
      "0.0001220703125\n",
      "6.103515625e-05\n",
      "3.0517578125e-05\n",
      "1.52587890625e-05\n",
      "7.62939453125e-06\n",
      "3.814697265625e-06\n",
      "1.9073486328125e-06\n",
      "9.5367431640625e-07\n",
      "4.76837158203125e-07\n",
      "2.384185791015625e-07\n",
      "1.1920928955078125e-07\n",
      "5.960464477539063e-08\n",
      "[-132.25775344 2702.16337127] [-8.7075343e-06  1.6230108e-04]\n",
      "iter = 2; M0 = 0.020322436268492387; M1 = 0.001144599268958216; Loss = 0.3631812199547151\n",
      "iter = 3; M0 = 0.020321605375169496; M1 = 0.001145848688578427; Loss = 0.36315898320561846\n",
      "iter = 4; M0 = 0.02032041543572201; M1 = 0.0011460587882240014; Loss = 0.3631455623529033\n",
      "iter = 5; M0 = 0.02030040363370411; M1 = 0.0011472459471442793; Loss = 0.36296942065967674\n",
      "iter = 6; M0 = 0.020258439131678112; M1 = 0.0011471754108826988; Loss = 0.3626448951482938\n",
      "iter = 7; M0 = 0.020109786822469444; M1 = 0.0011428510303172449; Loss = 0.3615613219318648\n",
      "iter = 8; M0 = 0.019709782920211878; M1 = 0.001125042254132247; Loss = 0.35865172167307346\n",
      "iter = 9; M0 = 0.018411882301995607; M1 = 0.0010565617893579199; Loss = 0.3489022412353777\n",
      "iter = 10; M0 = 0.01382637706657381; M1 = 0.0007953532710124145; Loss = 0.31139831437963883\n",
      "0.5\n",
      "0.25\n",
      "iter = 11; M0 = 0.009388348435607492; M1 = 0.0005346860235786908; Loss = 0.2726970009920689\n",
      "iter = 12; M0 = 0.0035220288681780468; M1 = 0.00018261408839537045; Loss = 0.06180082131006611\n",
      "0.5\n",
      "0.25\n",
      "0.125\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0078125\n",
      "iter = 13; M0 = 0.0029919296379715236; M1 = 0.00015036339524118346; Loss = 0.00955031880183237\n",
      "0.5\n",
      "0.25\n",
      "iter = 14; M0 = 0.0030341148859931912; M1 = 0.0001519057388259577; Loss = 0.007661415260409195\n",
      "0.5\n",
      "0.25\n",
      "iter = 15; M0 = 0.003997156118809415; M1 = 0.00020384871260067342; Loss = 0.0052216674617166\n",
      "iter = 16; M0 = 0.0041617996917380565; M1 = 0.0002086980589614746; Loss = 0.0010775699982018552\n",
      "iter = 17; M0 = 0.0040182724871503945; M1 = 0.00020116489067580897; Loss = 0.0009159188004598752\n",
      "iter = 18; M0 = 0.003922119148251363; M1 = 0.00019599994944190338; Loss = 0.0008822326391441394\n",
      "iter = 19; M0 = 0.003911731778203387; M1 = 0.00019535129813910324; Loss = 0.000876586690793828\n",
      "0.5\n",
      "iter = 20; M0 = 0.0039052414146595075; M1 = 0.00019488450891187343; Loss = 0.0008719726310036154\n",
      "0.5\n",
      "0.25\n",
      "iter = 21; M0 = 0.0039035208357005966; M1 = 0.0001947594663569202; Loss = 0.0008712002677595702\n",
      "0.5\n",
      "0.25\n",
      "iter = 22; M0 = 0.0039021458698214272; M1 = 0.00019466011601896674; Loss = 0.0008707441754399568\n",
      "0.5\n",
      "0.25\n",
      "0.125\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0078125\n",
      "0.00390625\n",
      "0.001953125\n",
      "0.0009765625\n",
      "0.00048828125\n",
      "0.000244140625\n",
      "0.0001220703125\n",
      "6.103515625e-05\n",
      "3.0517578125e-05\n",
      "1.52587890625e-05\n",
      "7.62939453125e-06\n",
      "3.814697265625e-06\n",
      "1.9073486328125e-06\n",
      "9.5367431640625e-07\n",
      "4.76837158203125e-07\n",
      "2.384185791015625e-07\n",
      "1.1920928955078125e-07\n",
      "5.960464477539063e-08\n",
      "2.9802322387695312e-08\n",
      "1.4901161193847656e-08\n",
      "7.450580596923828e-09\n",
      "3.725290298461914e-09\n",
      "1.862645149230957e-09\n",
      "9.313225746154785e-10\n",
      "4.656612873077393e-10\n",
      "2.3283064365386963e-10\n",
      "1.1641532182693481e-10\n",
      "5.820766091346741e-11\n",
      "2.9103830456733704e-11\n",
      "1.4551915228366852e-11\n",
      "7.275957614183426e-12\n",
      "3.637978807091713e-12\n",
      "1.8189894035458565e-12\n",
      "9.094947017729282e-13\n",
      "4.547473508864641e-13\n",
      "2.2737367544323206e-13\n",
      "1.1368683772161603e-13\n",
      "5.684341886080802e-14\n",
      "2.842170943040401e-14\n",
      "1.4210854715202004e-14\n",
      "7.105427357601002e-15\n",
      "3.552713678800501e-15\n",
      "1.7763568394002505e-15\n",
      "8.881784197001252e-16\n",
      "4.440892098500626e-16\n",
      "2.220446049250313e-16\n",
      "1.1102230246251565e-16\n",
      "5.551115123125783e-17\n",
      "2.7755575615628914e-17\n",
      "1.3877787807814457e-17\n",
      "6.938893903907228e-18\n",
      "3.469446951953614e-18\n",
      "1.734723475976807e-18\n",
      "8.673617379884035e-19\n",
      "4.336808689942018e-19\n",
      "2.168404344971009e-19\n",
      "1.0842021724855044e-19\n",
      "5.421010862427522e-20\n",
      "2.710505431213761e-20\n",
      "1.3552527156068805e-20\n",
      "6.776263578034403e-21\n",
      "3.3881317890172014e-21\n",
      "1.6940658945086007e-21\n",
      "8.470329472543003e-22\n",
      "4.235164736271502e-22\n",
      "2.117582368135751e-22\n",
      "1.0587911840678754e-22\n"
     ]
    }
   ],
   "source": [
    "##### INITIAL GUESS #####\n",
    "M0 = 0.02\n",
    "M1 = 0.01\n",
    "write_state(M0, M1)\n",
    "\n",
    "MAX_ITERS = 50\n",
    "\n",
    "\n",
    "\n",
    "state = np.array([M0, M1])\n",
    "gradient = np.array(eval_gradient(M0, M1))\n",
    "\n",
    "B = np.eye(2, dtype = float)\n",
    "B_inv = np.eye(2, dtype = float)\n",
    "\n",
    "for iteration in range(MAX_ITERS):\n",
    "    \n",
    "    state_old = np.copy(state)\n",
    "    gradient_old = np.copy(gradient)\n",
    "    \n",
    "    rho = -np.matmul(B_inv, gradient_old)\n",
    "    alpha = search_alpha(state_old, gradient_old, rho)\n",
    "    \n",
    "    s = alpha*rho\n",
    "    state = state_old + s\n",
    "    gradient = eval_gradient(state[0], state[1])\n",
    "    y = gradient - gradient_old\n",
    "        \n",
    "    if iteration == 0:\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    elif iteration == 1 or np.inner(y, s) <= 0:\n",
    "        \n",
    "        if (iteration == 1): print(y, s)\n",
    "        #print(f\"B reset iter {iteration}\")\n",
    "        B =  np.inner(y, y) / np.abs(np.inner(y, s)) * np.eye(2, dtype = float)\n",
    "        B_inv =  np.abs(np.inner(y, s)) / np.inner(y, y) * np.eye(2, dtype = float)\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        B, B_inv = BFGS_update_matrix(B, B_inv, y, s)\n",
    "    \n",
    "    print(f\"iter = {iteration+1}; M0 = {state[0]}; M1 = {state[1]}; Loss = {eval_loss(state[0], state[1])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-newton.pdf           \n",
    "https://www.stat.cmu.edu/~ryantibs/convexopt-F18/lectures/quasi-newton.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_BFGS_update(k, state_k, y_list, s_list, m):\n",
    "    \n",
    "    q = - np.array(eval_gradient(state_k[0], state_k[1]))\n",
    "    \n",
    "    alpha = np.zeros(k)\n",
    "    \n",
    "    for i in range(k-1, np.maximum(k-m, 0) - 1, -1):\n",
    "        \n",
    "        alpha[i] = np.inner(s_list[i-np.maximum(k-m, 0)], q)/np.inner(y_list[i-np.maximum(k-m, 0)], s_list[i-np.maximum(k-m, 0)])\n",
    "        q = q - alpha[i]*y_list[i-np.maximum(k-m, 0)]\n",
    "        \n",
    "    p = np.inner(np.array(y_list[-1]), np.array(s_list[-1]))/np.inner(np.array(y_list[-1]), np.array(y_list[-1]))*q\n",
    "    \n",
    "    for i in range(np.maximum(k-m, 0), k):\n",
    "        beta = np.inner(y_list[i - np.maximum(k-m, 0)], p)/np.inner(y_list[i - np.maximum(k-m, 0)], s_list[i - np.maximum(k-m, 0)])\n",
    "        print(i, k-m, np.maximum(k-m, 0))\n",
    "        p = p + (alpha[i] - beta)*s_list[i - np.maximum(k-m, 0)]\n",
    "        \n",
    "    return np.array(p)\n",
    "\n",
    "def manage_y_s_lists(m, y_list, s_list, y, s):\n",
    "    if len(y_list) < m and len(s_list) < m and len(y_list) == len(s_list):\n",
    "        y_list.append(np.array(y))\n",
    "        s_list.append(np.array(s))\n",
    "    elif len(y_list) == m and len(s_list) == m:\n",
    "        y_list.pop(0)\n",
    "        s_list.pop(0)\n",
    "        y_list.append(np.array(y))\n",
    "        s_list.append(np.array(s))\n",
    "        \n",
    "    return y_list, s_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 1; M0 = 0.02033114380278951; M1 = 0.0009822981888490488; Loss = 0.4963958949023225\n",
      "0 -9 0\n",
      "iter = 2; M0 = 0.020325979150718442; M1 = 0.0011227234332402186; Loss = 0.2880680156888957\n",
      "0 -8 0\n",
      "1 -8 0\n",
      "iter = 3; M0 = 0.020325824419222017; M1 = 0.0011220775201944832; Loss = 0.2880566727551021\n",
      "0 -7 0\n",
      "1 -7 0\n",
      "2 -7 0\n",
      "iter = 4; M0 = 0.020325672668254635; M1 = 0.0011222171620638892; Loss = 0.2880526316434578\n",
      "0 -6 0\n",
      "1 -6 0\n",
      "2 -6 0\n",
      "3 -6 0\n",
      "iter = 5; M0 = 0.020325070760934736; M1 = 0.0011223733248948327; Loss = 0.2880449403181858\n",
      "0 -5 0\n",
      "1 -5 0\n",
      "2 -5 0\n",
      "3 -5 0\n",
      "4 -5 0\n",
      "iter = 6; M0 = 0.02032314357054004; M1 = 0.001122542733858776; Loss = 0.2880258151257073\n",
      "0 -4 0\n",
      "1 -4 0\n",
      "2 -4 0\n",
      "3 -4 0\n",
      "4 -4 0\n",
      "5 -4 0\n",
      "iter = 7; M0 = 0.020317072133122147; M1 = 0.0011226523928726418; Loss = 0.287972444275405\n",
      "0 -3 0\n",
      "1 -3 0\n",
      "2 -3 0\n",
      "3 -3 0\n",
      "4 -3 0\n",
      "5 -3 0\n",
      "6 -3 0\n",
      "iter = 8; M0 = 0.020299760409003442; M1 = 0.0011223529542118914; Loss = 0.28782816048366766\n",
      "0 -2 0\n",
      "1 -2 0\n",
      "2 -2 0\n",
      "3 -2 0\n",
      "4 -2 0\n",
      "5 -2 0\n",
      "6 -2 0\n",
      "7 -2 0\n",
      "iter = 9; M0 = 0.020249323027102698; M1 = 0.0011205130406593798; Loss = 0.2874119678567223\n",
      "0 -1 0\n",
      "1 -1 0\n",
      "2 -1 0\n",
      "3 -1 0\n",
      "4 -1 0\n",
      "5 -1 0\n",
      "6 -1 0\n",
      "7 -1 0\n",
      "8 -1 0\n",
      "iter = 10; M0 = 0.02009845284935823; M1 = 0.0011133879566229091; Loss = 0.28612783609032133\n",
      "0 0 0\n",
      "1 0 0\n",
      "2 0 0\n",
      "3 0 0\n",
      "4 0 0\n",
      "5 0 0\n",
      "6 0 0\n",
      "7 0 0\n",
      "8 0 0\n",
      "9 0 0\n",
      "iter = 11; M0 = 0.019577428241235713; M1 = 0.0010858263682855872; Loss = 0.2812551308449522\n",
      "1 1 1\n",
      "2 1 1\n",
      "3 1 1\n",
      "4 1 1\n",
      "5 1 1\n",
      "6 1 1\n",
      "7 1 1\n",
      "8 1 1\n",
      "9 1 1\n",
      "10 1 1\n",
      "iter = 12; M0 = 0.016811923691400003; M1 = 0.0009334787969185574; Loss = 0.25162055614602064\n",
      "2 2 2\n",
      "3 2 2\n",
      "4 2 2\n",
      "5 2 2\n",
      "6 2 2\n",
      "7 2 2\n",
      "8 2 2\n",
      "9 2 2\n",
      "10 2 2\n",
      "11 2 2\n",
      "iter = 13; M0 = 0.006512520829923752; M1 = 0.0003606250452069167; Loss = 0.1636737718479168\n",
      "3 3 3\n",
      "4 3 3\n",
      "5 3 3\n",
      "6 3 3\n",
      "7 3 3\n",
      "8 3 3\n",
      "9 3 3\n",
      "10 3 3\n",
      "11 3 3\n",
      "12 3 3\n",
      "iter = 14; M0 = 0.0071123640959238595; M1 = 0.0003916647016420037; Loss = 0.1503730504931264\n",
      "4 4 4\n",
      "5 4 4\n",
      "6 4 4\n",
      "7 4 4\n",
      "8 4 4\n",
      "9 4 4\n",
      "10 4 4\n",
      "11 4 4\n",
      "12 4 4\n",
      "13 4 4\n",
      "iter = 15; M0 = 0.005070036856158741; M1 = 0.0002760719165317019; Loss = 0.1464398780075976\n",
      "5 5 5\n",
      "6 5 5\n",
      "7 5 5\n",
      "8 5 5\n",
      "9 5 5\n",
      "10 5 5\n",
      "11 5 5\n",
      "12 5 5\n",
      "13 5 5\n",
      "14 5 5\n",
      "iter = 16; M0 = 0.0057388665987800555; M1 = 0.0003117469477635636; Loss = 0.1395918335646443\n",
      "6 6 6\n",
      "7 6 6\n",
      "8 6 6\n",
      "9 6 6\n",
      "10 6 6\n",
      "11 6 6\n",
      "12 6 6\n",
      "13 6 6\n",
      "14 6 6\n",
      "15 6 6\n",
      "iter = 17; M0 = 0.005745471034627709; M1 = 0.0003097785256166657; Loss = 0.13453704679123238\n",
      "7 7 7\n",
      "8 7 7\n",
      "9 7 7\n",
      "10 7 7\n",
      "11 7 7\n",
      "12 7 7\n",
      "13 7 7\n",
      "14 7 7\n",
      "15 7 7\n",
      "16 7 7\n",
      "iter = 18; M0 = 0.005212509135745867; M1 = 0.00027153410389062037; Loss = 0.050316190547589214\n",
      "8 8 8\n",
      "9 8 8\n",
      "10 8 8\n",
      "11 8 8\n",
      "12 8 8\n",
      "13 8 8\n",
      "14 8 8\n",
      "15 8 8\n",
      "16 8 8\n",
      "17 8 8\n",
      "iter = 19; M0 = 0.004234775033517437; M1 = 0.0002068419695145686; Loss = 0.04482553873335559\n",
      "9 9 9\n",
      "10 9 9\n",
      "11 9 9\n",
      "12 9 9\n",
      "13 9 9\n",
      "14 9 9\n",
      "15 9 9\n",
      "16 9 9\n",
      "17 9 9\n",
      "18 9 9\n",
      "iter = 20; M0 = 0.005003799439870933; M1 = 0.00025756215900158325; Loss = 0.01316064041611174\n",
      "10 10 10\n",
      "11 10 10\n",
      "12 10 10\n",
      "13 10 10\n",
      "14 10 10\n",
      "15 10 10\n",
      "16 10 10\n",
      "17 10 10\n",
      "18 10 10\n",
      "19 10 10\n",
      "iter = 21; M0 = 0.00469984170108588; M1 = 0.00023785007796716276; Loss = 0.0026650748656380317\n",
      "11 11 11\n",
      "12 11 11\n",
      "13 11 11\n",
      "14 11 11\n",
      "15 11 11\n",
      "16 11 11\n",
      "17 11 11\n",
      "18 11 11\n",
      "19 11 11\n",
      "20 11 11\n",
      "iter = 22; M0 = 0.004559678936886147; M1 = 0.0002304726272551001; Loss = 0.002108477397258395\n",
      "12 12 12\n",
      "13 12 12\n",
      "14 12 12\n",
      "15 12 12\n",
      "16 12 12\n",
      "17 12 12\n",
      "18 12 12\n",
      "19 12 12\n",
      "20 12 12\n",
      "21 12 12\n",
      "iter = 23; M0 = 0.003858876550906335; M1 = 0.00019351403952747555; Loss = 0.0010842079562998564\n",
      "13 13 13\n",
      "14 13 13\n",
      "15 13 13\n",
      "16 13 13\n",
      "17 13 13\n",
      "18 13 13\n",
      "19 13 13\n",
      "20 13 13\n",
      "21 13 13\n",
      "22 13 13\n",
      "iter = 24; M0 = 0.004006028063997453; M1 = 0.0002011867856653472; Loss = 0.0009991447876427557\n",
      "14 14 14\n",
      "15 14 14\n",
      "16 14 14\n",
      "17 14 14\n",
      "18 14 14\n",
      "19 14 14\n",
      "20 14 14\n",
      "21 14 14\n",
      "22 14 14\n",
      "23 14 14\n",
      "iter = 25; M0 = 0.003993637077670818; M1 = 0.00020044473338280247; Loss = 0.000977441895135371\n",
      "15 15 15\n",
      "16 15 15\n",
      "17 15 15\n",
      "18 15 15\n",
      "19 15 15\n",
      "20 15 15\n",
      "21 15 15\n",
      "22 15 15\n",
      "23 15 15\n",
      "24 15 15\n",
      "iter = 26; M0 = 0.003918903470012316; M1 = 0.00019573333292052046; Loss = 0.0008766029182836291\n",
      "16 16 16\n",
      "17 16 16\n",
      "18 16 16\n",
      "19 16 16\n",
      "20 16 16\n",
      "21 16 16\n",
      "22 16 16\n",
      "23 16 16\n",
      "24 16 16\n",
      "25 16 16\n",
      "iter = 27; M0 = 0.003910678002084838; M1 = 0.0001951930685914097; Loss = 0.0008725098488484484\n",
      "17 17 17\n",
      "18 17 17\n",
      "19 17 17\n",
      "20 17 17\n",
      "21 17 17\n",
      "22 17 17\n",
      "23 17 17\n",
      "24 17 17\n",
      "25 17 17\n",
      "26 17 17\n",
      "iter = 28; M0 = 0.003904575721573467; M1 = 0.00019479239631903776; Loss = 0.0008707888786742765\n",
      "18 18 18\n",
      "19 18 18\n",
      "20 18 18\n",
      "21 18 18\n",
      "22 18 18\n",
      "23 18 18\n",
      "24 18 18\n",
      "25 18 18\n",
      "26 18 18\n",
      "27 18 18\n",
      "iter = 29; M0 = 0.003903752924521254; M1 = 0.00019473839121223843; Loss = 0.0008706401765452077\n",
      "19 19 19\n",
      "20 19 19\n",
      "21 19 19\n",
      "22 19 19\n",
      "23 19 19\n",
      "24 19 19\n",
      "25 19 19\n",
      "26 19 19\n",
      "27 19 19\n",
      "28 19 19\n"
     ]
    }
   ],
   "source": [
    "##### INITIAL GUESS #####\n",
    "M0 = 0.02\n",
    "M1 = 0.01\n",
    "write_state(M0, M1)\n",
    "\n",
    "MAX_ITERS = 50\n",
    "\n",
    "\n",
    "\n",
    "state = np.array([M0, M1])\n",
    "gradient = np.array(eval_gradient(M0, M1))\n",
    "\n",
    "B = np.eye(2, dtype = float)\n",
    "B_inv = np.eye(2, dtype = float)\n",
    "\n",
    "s_list = []\n",
    "y_list = []\n",
    "\n",
    "m = 10\n",
    "\n",
    "for iteration in range(MAX_ITERS):\n",
    "    state_old = np.copy(state)\n",
    "    gradient_old = np.copy(gradient)\n",
    "    \n",
    "    if iteration == 0:\n",
    "        rho = -np.matmul(B_inv, gradient_old)\n",
    "        \n",
    "    else: \n",
    "\n",
    "        rho = L_BFGS_update(iteration, state_old, y_list, s_list, m)\n",
    "        \n",
    "    alpha = search_alpha(state_old, gradient_old, rho)\n",
    "    s = alpha*rho\n",
    "    state = state_old + s\n",
    "    gradient = eval_gradient(state[0], state[1])\n",
    "    y = gradient - gradient_old\n",
    "        \n",
    "    y_list, s_list = manage_y_s_lists(m, y_list, s_list, y, s)\n",
    "    \n",
    "    print(f\"iter = {iteration+1}; M0 = {state[0]}; M1 = {state[1]}; Loss = {eval_loss(state[0], state[1])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
